'''
Albion Online Refining Data Retrieval Module

This module provides functions to retrieve refining data for various resources 
from the Albion Data Project API, generate API URLs, and save the data to CSV files.

Albion Data Project: https://www.albion-online-data.com
'''

import os
import requests
import pandas as pd
from bs4 import BeautifulSoup

def generate_refining_api_variables(raw_resource_type : str, refined_resource_type : str):
    '''
    Use args in order to generate the correct variables for the api, they need to be paired

    ARGS:
        - raw_resource |str| -- WOOD, ORE, FIBER, HIDE
        - refined_resource |str| -- PLANKS, METALBAR, CLOTH, LEATHER

    Return:
        - refining_mats_variables |list of strings| -- Variables to generate url for api
    
    '''

    # Define the tiers and enchantments
    tier = ('T4', 'T5', 'T6', 'T7', 'T8')
    enchantment = (0, 1, 2, 3, 4)
    raw_resource_items = []
    refined_resource_items = []
    refined_resource_items.append(f'T3_{refined_resource_type}') # add T3 refined resource

    for t_level in tier:
        # Flag to track if unenchated raw resource has been added for the tier
        unenchanted_raw_added = False
        unenchanted_refined_added = False
        for e_level in enchantment:
            if not unenchanted_raw_added:
                raw_resource_items.append(f'{t_level}_{raw_resource_type}')
                # Set the constant to True after adding the unenchated tier resource
                unenchanted_raw_added = True
                if not unenchanted_refined_added:
                    refined_resource_items.append(f'{t_level}_{refined_resource_type}')
                unenchanted_refined_added = True
            if e_level > 0:
                raw_resource_items.append(
                    f'{t_level}_{raw_resource_type}_LEVEL{e_level}@{e_level}')
                refined_resource_items.append(
                    f'{t_level}_{refined_resource_type}_LEVEL{e_level}@{e_level}')

    refining_items_required = raw_resource_items + refined_resource_items
    return refining_items_required

def generate_refining_api_url(refining_items_required: str):
    '''
    Generate the API URL based on the list of refining items required.

    Args:
        - refining_items_required |str| -- Variable for constructing the URL.

    Returns:
        -api_url |str| -- The final API URL.
    '''


    table_view_base_url = 'https://west.albion-online-data.com/api/v2/stats/view/'
    cities = 'Martlock,Fort%20Sterling,Thetford,Lymhurst,Bridgewatch'

    table_view_api_url = (
    f'{table_view_base_url}{",".join(refining_items_required)}'
    f'?locations={cities}'
)

    return table_view_api_url

def create_csv_file(api_url: str, csv_filename: str, timeout: int = 10):
    '''
    Fetch data from an API and save it to a CSV file.

    Args:
        - api_url |str| -- The API URL generated by generate_refining_api_variables().
        - csv_filename |str| -- Define the CSV file path with .csv extension for storage.
        - timeout |int, optional| -- Maximum response wait time in seconds (default: 10 seconds).

    Result:
        - Creates a CSV file with the specified data.
    '''

    # Define the selected columns here
    selected_columns = ['item_id', 'city',
        'sell_price_min', 'sell_price_min_date',
        'sell_price_max', 'sell_price_max_date']

    try:
        response = requests.get(api_url, timeout=timeout)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table')

            if table:
                data_frame  = pd.read_html(str(table))[0]
                data_frame  = data_frame [selected_columns]
                data_frame .to_csv(csv_filename, index=False, encoding='utf-8')

                print(f'Data has been saved to {csv_filename}')
            else:
                print('No table is found in the API response')
        else:
            print(f'Failed to fetch data from the API. Status code: {response.status_code}')

    except requests.exceptions.Timeout:
        print('The request to the API timed out.')


# TODO: add a function that executes this either on a timer or on specific request
def main():
    script_directory = os.path.dirname(os.path.abspath(__file__))

    csv_folder = os.path.join(script_directory, 'csv_data')
    os.makedirs(csv_folder, exist_ok=True)

    wood_api_variables = generate_refining_api_variables('WOOD', 'PLANKS')
    ore_api_variables = generate_refining_api_variables('ORE', 'METALBAR')
    cloth_api_variables = generate_refining_api_variables('FIBER', 'CLOTH')
    hide_api_variables = generate_refining_api_variables('HIDE', 'LEATHER')


    wood_api_url = generate_refining_api_url(wood_api_variables)
    ore_api_url = generate_refining_api_url(ore_api_variables)
    cloth_api_url = generate_refining_api_url(cloth_api_variables)
    hide_api_url = generate_refining_api_url(hide_api_variables)


    csv_filename_wood = os.path.join(csv_folder, 'wood_refining.csv')
    csv_filename_ore = os.path.join(csv_folder, 'ore_refining.csv')
    csv_filename_cloth = os.path.join(csv_folder, 'cloth_refining.csv')
    csv_filename_fiber = os.path.join(csv_folder, 'hide_refining.csv')

    create_csv_file(wood_api_url, csv_filename_wood)
    create_csv_file(ore_api_url, csv_filename_ore)
    create_csv_file(cloth_api_url, csv_filename_cloth)
    create_csv_file(hide_api_url, csv_filename_fiber)

if __name__ == "__main__":
    main()
